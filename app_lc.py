import datetime
import json
import logging
import os
from pathlib import Path

import click
from dotenv import load_dotenv

from src.build_index import KnowledgeDB
from src.bot import CliBot
from src.orders_db import load_orders
from src.prompts.style_config import StyleConfig
from src.style_eval import BotStyleEvaluator
from src.rag_eval import RAGEvaluator

load_dotenv()


class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "message": record.getMessage(),
        }
        token_usage = getattr(record, "token_usage", None)
        if token_usage:
            log_record["token_usage"] = token_usage
        return json.dumps(log_record, ensure_ascii=False)


timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f"./logs/session_{timestamp}.jsonl"

handler = logging.FileHandler(log_filename, encoding="utf-8")
handler.setFormatter(JsonFormatter(datefmt="%Y-%m-%d %H:%M:%S"))

logging.basicConfig(level=logging.INFO, handlers=[handler], force=True)

KNOWLEDGE_DOCS = [
        Path("./data/policy_returns.pdf"),
        Path("./data/shipping_terms.pdf"),
    ]

DB_PATH = Path("./vectordb")


def get_common_config():
    """Get common configuration for both bot and evaluate commands."""
    model_name = os.getenv("API_MODEL", "gpt-4o-mini")
    api_key = os.getenv("API_KEY")
    api_url = os.getenv("API_URL", "https://openrouter.ai/api/v1")
    
    if not api_key:
        raise ValueError("API_KEY is not set")
    
    person_name = os.getenv("PERSON_NAME", "alex")

    # Load person configuration
    person = StyleConfig.load(person_name, "./data/style_guide.yaml")

    # Load orders data
    load_orders()

    return {"model_name": model_name, "api_key": api_key, "api_url": api_url, "person": person}


@click.group()
def main():
    """E-commerce bot CLI with multiple modes."""
    pass


@main.command()
def bot():
    """Run the bot in interactive mode."""
    config = get_common_config()

    knowledge_db = KnowledgeDB(KNOWLEDGE_DOCS, DB_PATH)
    vector_store = knowledge_db.get_vector_store()

    bot = CliBot(
        model_name=config["model_name"],
        api_key=config["api_key"],
        api_url=config["api_url"],
        person=config["person"],
        vector_store=vector_store,
    )

    logging.info("=== New session ===")
    bot("user_123")


@main.command()
@click.option("--eval-model", default="gpt-4o-mini", help="Model to use for evaluation")
def evaluate_style(eval_model):
    """Run the bot in evaluation mode."""
    config = get_common_config()    

    knowledge_db = KnowledgeDB(KNOWLEDGE_DOCS, DB_PATH)
    vector_store = knowledge_db.get_vector_store()

    bot = CliBot(
        model_name=config["model_name"],
        api_key=config["api_key"],
        api_url=config["api_url"],
        person=config["person"],
        vector_store=vector_store,
        silent=True,
    )

    reports_dir = Path("reports")
    reports_dir.mkdir(exist_ok=True)

    evaluator = BotStyleEvaluator(
        model_name=eval_model,
        api_key=config["api_key"],
        api_url=config["api_url"],
        person=config["person"],
        reports_dir=reports_dir,
        bot=bot,
    )

    data_dir = Path("data")
    eval_prompts = (
        (data_dir / "eval_style_prompts.txt").read_text(encoding="utf-8").strip().splitlines()
    )

    report = evaluator.eval_batch(eval_prompts)

    summary = report["summary"]
    
    print("=" * 50)
    print("–û–¶–ï–ù–ö–ê –°–¢–ò–õ–Ø - –°–í–û–î–ö–ê")
    print("=" * 50)
    print(f"üìä –û–±—â–∏–π —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: {summary['mean_final']:.2f}/100")
    print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è: {summary['pass_rate']:.2f}%")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏: {summary['successful_evaluations']}/{summary['total_cases']}")
    print(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏: {summary['failed_evaluations']}")
    
    print("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"  ‚Ä¢ –ü—Ä–∞–≤–∏–ª–∞ (rule-based): {summary['mean_rule_score']:.2f}")
    print(f"  ‚Ä¢ –ò–ò –æ—Ü–µ–Ω–∫–∞ (LLM-based): {summary['mean_llm_score']:.2f}")
    print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {summary['std_final']:.2f}")
    
    print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –û–¶–ï–ù–û–ö:")
    print(f"  ‚Ä¢ –ú–∏–Ω–∏–º—É–º: {summary['min_final']}")
    print(f"  ‚Ä¢ 25-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å: {summary['p25_final']}")
    print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {summary['median_final']}")
    print(f"  ‚Ä¢ 75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å: {summary['p75_final']}")
    print(f"  ‚Ä¢ 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å: {summary['p95_final']}")
    print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: {summary['max_final']}")
    
    if summary.get('violations_count', 0) > 0:
        print(f"\n‚ö†Ô∏è  –ù–ê–†–£–®–ï–ù–ò–Ø –ü–†–ê–í–ò–õ:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –Ω–∞—Ä—É—à–µ–Ω–∏–π: {summary['violations_count']}")
        common_violations = summary.get('common_violations', {})
        if common_violations:
            for violation, count in list(common_violations.items())[:5]:
                print(f"  ‚Ä¢ {violation}: {count} —Ä–∞–∑(–∞)")
    
    print(f"\nüìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {reports_dir / 'style_eval.json'}")
    print(f"üìã –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞: {reports_dir / 'style_eval_summary.json'}")
    print("=" * 50)


@main.command()
def evaluate_rag():
    """Run RAG evaluation mode to test document-based question answering."""
    config = get_common_config()

    knowledge_db = KnowledgeDB(KNOWLEDGE_DOCS, DB_PATH)
    vector_store = knowledge_db.get_vector_store()

    bot = CliBot(
        model_name=config["model_name"],
        api_key=config["api_key"],
        api_url=config["api_url"],
        person=config["person"],
        vector_store=vector_store,
        silent=True,
    )

    reports_dir = Path("reports")
    reports_dir.mkdir(exist_ok=True)

    evaluator = RAGEvaluator(
        bot=bot,
        reports_dir=reports_dir,
    )

    data_dir = Path("data")
    eval_prompts_file = data_dir / "eval_rag_prompts.json"

    report = evaluator.evaluate(eval_prompts_file)

    # Print summary
    print("=" * 50)
    print("RAG –û–¶–ï–ù–ö–ê - –°–í–û–î–ö–ê")
    print("=" * 50)
    print(f"üìä –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è: {report['pass_rate']:.2f}%")
    print(f"üéØ –¶–µ–ª–µ–≤–æ–π –ø–æ—Ä–æ–≥: {report['target_pass_rate']:.2f}%")
    print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {report['passed_count']}/{report['total_count']}")
    print(f"‚ùå –ù–µ –ø—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {report['failed_count']}")
    
    if report['meets_target']:
        print(f"\n‚úÖ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è >= {report['target_pass_rate']}%")
    else:
        print(f"\n‚ùå –¶–ï–õ–¨ –ù–ï –î–û–°–¢–ò–ì–ù–£–¢–ê: –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è < {report['target_pass_rate']}%")
    
    # Show breakdown by category
    categories = {}
    for item in report['items']:
        category = item.get('category', 'unknown')
        if category not in categories:
            categories[category] = {'total': 0, 'passed': 0}
        categories[category]['total'] += 1
        if item['pass']:
            categories[category]['passed'] += 1
    
    if categories:
        print(f"\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
        for category, stats in sorted(categories.items()):
            pass_rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
            print(f"  ‚Ä¢ {category}: {stats['passed']}/{stats['total']} ({pass_rate:.1f}%)")
    
    # Show failed tests
    failed_tests = [item for item in report['items'] if not item['pass']]
    if failed_tests:
        print(f"\n‚ùå –ù–ï–£–î–ê–ß–ù–´–ï –¢–ï–°–¢–´ ({len(failed_tests)}):")
        for i, item in enumerate(failed_tests, 1):
            oos_text = "OOS" if item['oos'] else "in-scope"
            print(f"  {i}. [{oos_text}] {item['q']}")
            print(f"     –ü—Ä–∏—á–∏–Ω–∞: {item.get('reason', 'N/A')}")
    
    print(f"\nüìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {reports_dir / 'rag_eval.json'}")
    print("=" * 50)


if __name__ == "__main__":
    main()